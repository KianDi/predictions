{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How similar is the text?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, spacy\n",
    "\n",
    "import pandas as pd\n",
    "# import gensim.downloader as api\n",
    "\n",
    "# Get the current working directory of the notebook\n",
    "notebook_dir = os.getcwd()\n",
    "# Add the parent directory to the system path\n",
    "sys.path.append(os.path.join(notebook_dir, '../'))\n",
    "\n",
    "from feature_extraction import SpacyFeatureExtraction\n",
    "# from data_processing import DataProcessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load_nlp_model = spacy.load(\"en_core_web_md\")\n",
    "# pretrained_word_two_vec_model = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mappings = {\n",
    "    \"I predict it will be sunny on 4 April 2025.\" : [\"It was sunny on 4 April 2025.\", \"It was not sunny on 4 April 2025.\"],\n",
    "    \"Apple's stock will rise by 10%.\" : [\"Apple's stock rose by 11%.\", \"Apple's stock rose by 900%.\", \"Apple's stock rose by 10%.\"]\n",
    "}\n",
    "\n",
    "mappings = {\n",
    "    \"Tesla's stock will rise by 10%.\" : [\"Apple's stock will rise by 10%.\", \"Apple's stock rose by 11%.\", \"Apple's stock rose by 10%.\", \"Apple's stock rose by 900%.\"]\n",
    "}\n",
    "\n",
    "mappings = {\n",
    "    \"Tesla's stock will rise by 10%.\" : [\"Apple's stock will rise by 10%.\", \"Apple's stock rose by 11%.\", \"Apple's stock rose by 10%.\", \"Apple's stock rose by 900%.\"],\n",
    "    \"Apple's stock will rise by 10%.\" : [\"Apple's stock will rise by 10%.\", \"Apple's stock rose by 11%.\", \"Apple's stock rose by 900%.\", \"Apple's stock rose by 10%.\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Tesla's stock will rise by 10%.\n",
      "    Observation: Apple's stock will rise by 10%.\n",
      "    Similarity: 0.9796735644340515\n",
      "\n",
      "    Observation: Apple's stock rose by 11%.\n",
      "    Similarity: 0.8603723645210266\n",
      "\n",
      "    Observation: Apple's stock rose by 10%.\n",
      "    Similarity: 0.8691259026527405\n",
      "\n",
      "    Observation: Apple's stock rose by 900%.\n",
      "    Similarity: 0.8487551808357239\n",
      "\n",
      "Prediction: Apple's stock will rise by 10%.\n",
      "    Observation: Apple's stock will rise by 10%.\n",
      "    Similarity: 1.0\n",
      "\n",
      "    Observation: Apple's stock rose by 11%.\n",
      "    Similarity: 0.8647664785385132\n",
      "\n",
      "    Observation: Apple's stock rose by 900%.\n",
      "    Similarity: 0.8541026711463928\n",
      "\n",
      "    Observation: Apple's stock rose by 10%.\n",
      "    Similarity: 0.8756678104400635\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/78/9z0b45fx1xqbwxh8vk97lcfh0000gn/T/ipykernel_88690/312081125.py:18: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  similarity = prediction_doc.similarity(observation_doc)\n"
     ]
    }
   ],
   "source": [
    "load_nlp_model = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "all_mappings = {} # Mapping of predictions to observations\n",
    "pom_mappings = {} # Mapping of predictions to observations to metrics/scoresl pos can be misleading\n",
    "\n",
    "oms = []\n",
    "for prediction in mappings.keys():\n",
    "    print(f\"Prediction: {prediction}\")\n",
    "\n",
    "    om_mappings = {} # Mapping of observations to metrics/scores\n",
    "\n",
    "    for observation in mappings[prediction]:\n",
    "        print(f\"    Observation: {observation}\")\n",
    "\n",
    "        # Calculate the similarity score\n",
    "        prediction_doc = load_nlp_model(prediction)\n",
    "        observation_doc = load_nlp_model(observation)\n",
    "        similarity = prediction_doc.similarity(observation_doc)\n",
    "        print(f\"    Similarity: {similarity}\")\n",
    "        om_mappings[observation] = [similarity]\n",
    "        print()   \n",
    "        # print(f\"    OM Mapping: {om_mappings}\")\n",
    "    pom_mappings[prediction] = om_mappings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pom_mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Prediction</th>\n",
       "      <th>Observation</th>\n",
       "      <th>Spacy Similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tesla's stock will rise by 10%.</td>\n",
       "      <td>Apple's stock will rise by 10%.</td>\n",
       "      <td>0.979674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tesla's stock will rise by 10%.</td>\n",
       "      <td>Apple's stock rose by 11%.</td>\n",
       "      <td>0.860372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tesla's stock will rise by 10%.</td>\n",
       "      <td>Apple's stock rose by 10%.</td>\n",
       "      <td>0.869126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tesla's stock will rise by 10%.</td>\n",
       "      <td>Apple's stock rose by 900%.</td>\n",
       "      <td>0.848755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Apple's stock will rise by 10%.</td>\n",
       "      <td>Apple's stock will rise by 10%.</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Apple's stock will rise by 10%.</td>\n",
       "      <td>Apple's stock rose by 11%.</td>\n",
       "      <td>0.864766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Apple's stock will rise by 10%.</td>\n",
       "      <td>Apple's stock rose by 900%.</td>\n",
       "      <td>0.854103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Apple's stock will rise by 10%.</td>\n",
       "      <td>Apple's stock rose by 10%.</td>\n",
       "      <td>0.875668</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Prediction                      Observation  \\\n",
       "0  Tesla's stock will rise by 10%.  Apple's stock will rise by 10%.   \n",
       "1  Tesla's stock will rise by 10%.       Apple's stock rose by 11%.   \n",
       "2  Tesla's stock will rise by 10%.       Apple's stock rose by 10%.   \n",
       "3  Tesla's stock will rise by 10%.      Apple's stock rose by 900%.   \n",
       "4  Apple's stock will rise by 10%.  Apple's stock will rise by 10%.   \n",
       "5  Apple's stock will rise by 10%.       Apple's stock rose by 11%.   \n",
       "6  Apple's stock will rise by 10%.      Apple's stock rose by 900%.   \n",
       "7  Apple's stock will rise by 10%.       Apple's stock rose by 10%.   \n",
       "\n",
       "   Spacy Similarity  \n",
       "0          0.979674  \n",
       "1          0.860372  \n",
       "2          0.869126  \n",
       "3          0.848755  \n",
       "4          1.000000  \n",
       "5          0.864766  \n",
       "6          0.854103  \n",
       "7          0.875668  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Flatten the dictionary into a list of rows\n",
    "data = []\n",
    "for prediction, observations in pom_mappings.items():\n",
    "    for observation, scores in observations.items():\n",
    "        data.append([prediction, observation, scores[0]])\n",
    "\n",
    "# Create the DataFrame\n",
    "df = pd.DataFrame(data, columns=['Prediction', 'Observation', 'Spacy Similarity'])\n",
    "\n",
    "# Display the DataFrame\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison\n",
    "1. No focus on #s, more on words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/78/9z0b45fx1xqbwxh8vk97lcfh0000gn/T/ipykernel_88690/1801851099.py:4: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  similarity = prediction_doc.similarity(observation_doc)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9251048564910889"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_doc = load_nlp_model(\"10%\")\n",
    "observation_doc = load_nlp_model(\"10.5%\")\n",
    "observation_doc_2 = load_nlp_model(\"11%\")\n",
    "similarity = prediction_doc.similarity(observation_doc)\n",
    "similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/78/9z0b45fx1xqbwxh8vk97lcfh0000gn/T/ipykernel_88690/4256136358.py:1: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  similarity = prediction_doc.similarity(observation_doc_2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9595414400100708"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity = prediction_doc.similarity(observation_doc_2)\n",
    "similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/78/9z0b45fx1xqbwxh8vk97lcfh0000gn/T/ipykernel_88690/3994610451.py:3: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  similarity = prediction_doc.similarity(observation_doc)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9899832606315613"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_doc = load_nlp_model(\"Apple's stock rose by 10%.\")\n",
    "observation_doc = load_nlp_model(\"Apple's stock rose by 11%.\")\n",
    "similarity = prediction_doc.similarity(observation_doc)\n",
    "similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-9.69052166e-02,  4.43707854e-02, -3.03838477e-02, -2.54174694e-04,\n",
       "        3.76424938e-01, -8.68998468e-03,  3.35592031e-01,  2.44690776e-01,\n",
       "        5.41072249e-01, -3.75136226e-01, -5.61025739e-02,  4.03797567e-01,\n",
       "       -3.96669120e-01,  6.21899724e-01, -1.49399251e-01,  4.20594633e-01,\n",
       "       -6.60013497e-01,  4.23254371e-02,  1.75270349e-01,  2.39882201e-01,\n",
       "       -3.27159762e-01,  2.70389676e-01,  1.91664666e-01, -1.41071290e-01,\n",
       "        4.56980765e-01, -2.14351714e-02,  1.68063223e-01, -1.47116721e-01,\n",
       "        2.89462171e-02,  2.59644151e-01,  1.40097201e-01, -3.19258392e-01,\n",
       "        1.61395982e-01, -5.15149355e-01, -2.74461329e-01,  1.34547725e-01,\n",
       "       -1.52158603e-01, -2.81526130e-02, -8.63277316e-02,  2.68475622e-01,\n",
       "        2.45270133e-01, -6.46607578e-03,  2.96593577e-01,  4.67894614e-01,\n",
       "        7.09100813e-02, -3.24479461e-01, -6.74477741e-02,  6.88518107e-01,\n",
       "        1.01036236e-01, -3.16862434e-01,  7.60792196e-02,  7.62369931e-02,\n",
       "        3.15949544e-02, -6.90139532e-01,  2.47799069e-01, -2.62490660e-01,\n",
       "        1.42434865e-01, -3.58416677e-01,  5.55610299e-01, -1.30157486e-01,\n",
       "       -5.24919748e-01, -7.23473370e-01, -4.98673320e-03, -3.13523740e-01,\n",
       "       -1.18560076e-01, -1.02858067e-01,  3.78484786e-01, -4.93497431e-01,\n",
       "       -2.11151287e-01,  2.50969142e-01,  3.33601609e-02,  3.54717970e-01,\n",
       "       -1.22083530e-01, -2.95684904e-01,  3.56512308e-01, -3.29778224e-01,\n",
       "       -3.71079296e-01, -1.56280696e-01, -1.32851273e-01, -1.89584970e-01,\n",
       "        1.77220300e-01, -1.22947946e-01, -2.78379619e-01,  1.55220568e-01,\n",
       "       -6.68327659e-02, -2.14364633e-01,  3.32736641e-01,  3.64676714e-02,\n",
       "       -3.62706393e-01,  6.26346111e-01, -3.81499141e-01, -7.64165968e-02,\n",
       "       -7.51008615e-02,  4.56430167e-02, -4.81737852e-02,  1.58989310e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdv = prediction_doc.vector # 10\n",
    "pdv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.1375021 ,  0.05036654, -0.0824471 , -0.09218727,  0.42592007,\n",
       "       -0.02057144,  0.3527472 ,  0.24258192,  0.588894  , -0.40792432,\n",
       "       -0.0078516 ,  0.41594055, -0.30752772,  0.6503702 , -0.10828097,\n",
       "        0.39248452, -0.726854  ,  0.08076884,  0.20747659,  0.31021383,\n",
       "       -0.31106994,  0.31786215,  0.10419345, -0.14996357,  0.45800018,\n",
       "        0.00623147,  0.10050573, -0.1621193 ,  0.03166858,  0.29230028,\n",
       "        0.16873193, -0.37811226,  0.21399517, -0.57316726, -0.24211697,\n",
       "        0.13694435, -0.1441578 , -0.05136697, -0.07773665,  0.2569706 ,\n",
       "        0.31111842, -0.05671661,  0.27724874,  0.45298147,  0.06111439,\n",
       "       -0.29012552, -0.06525761,  0.67409015,  0.1075652 , -0.33304486,\n",
       "       -0.01102823,  0.11045703, -0.02698915, -0.6804062 ,  0.29783314,\n",
       "       -0.1287547 ,  0.17033227, -0.34667575,  0.56826645, -0.13759255,\n",
       "       -0.5063458 , -0.75957036,  0.00584174, -0.26786044, -0.09881489,\n",
       "       -0.14349848,  0.46282774, -0.46586582, -0.25601786,  0.21176586,\n",
       "        0.15174815,  0.36985987, -0.16574626, -0.32093704,  0.30943295,\n",
       "       -0.37960425, -0.38040608, -0.14263228, -0.12378687, -0.2393227 ,\n",
       "        0.18215714, -0.0933006 , -0.31590974,  0.17470223, -0.08508855,\n",
       "       -0.17179835,  0.2753281 ,  0.05879234, -0.35054386,  0.56474394,\n",
       "       -0.4333668 , -0.10596386, -0.04418079, -0.00117675, -0.00655717,\n",
       "        0.11536697], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "odv = observation_doc.vector # 10.5\n",
    "odv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Five most popular similarity measures implementation in python](https://dataaspirant.com/five-most-popular-similarity-measures-implementation-in-python/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float32(0.99)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from math import*\n",
    "\n",
    "def square_rooted(x):\n",
    "\n",
    "    return round(sqrt(sum([a*a for a in x])),3)\n",
    "\n",
    "def cosine_similarity(x,y):\n",
    "\n",
    "    numerator = sum(a*b for a,b in zip(x,y))\n",
    "    denominator = square_rooted(x)*square_rooted(y)\n",
    "    return round(numerator/float(denominator),3)\n",
    "\n",
    "cosine_similarity(pdv, odv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "predictions",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
