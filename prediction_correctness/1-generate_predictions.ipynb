{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5fc9e2f4",
   "metadata": {},
   "source": [
    "# 1-Generate Predictions using LangChain\n",
    "\n",
    "- **Goal:** Prediction Recognition\n",
    "\n",
    "- **Purpose:** To implement step 1 with sub steps of prediction recognition pipeline. See steps\n",
    "    1. Generate predictions\n",
    "        1. Create several prediction prompts templates\n",
    "        2. Utilize open-source LLMs to generate predictions\n",
    "\n",
    "- **Misc:**\n",
    "    - `%store`: Cell magic will store the variable of interest so we can load in another notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01a3b011",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "from langchain_core.prompts import PipelinePromptTemplate, PromptTemplate\n",
    "\n",
    "# Get the current working directory of the notebook\n",
    "notebook_dir = os.getcwd()\n",
    "# Add the parent directory to the system path\n",
    "sys.path.append(os.path.join(notebook_dir, '../'))\n",
    "\n",
    "from log_files import LogData\n",
    "from data_processing import DataProcessing\n",
    "from text_generation_models import TextGenerationModelFactory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f573fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<text_generation_models.TextGenerationModelFactory object at 0x35125d2e0>\n"
     ]
    }
   ],
   "source": [
    "# pd.set_option('max_colwidth', 800)\n",
    "\n",
    "# groq_cloud_tgmf = TextGenerationModelFactory()\n",
    "# llama_versatile_generation_model = groq_cloud_tgmf.create_instance(model_name='llama-3.3-70b-versatile')\n",
    "# llama_instant_generation_model = groq_cloud_tgmf.create_instance('llama-3.1-8b-instant')\n",
    "# llama_70b_8192_generation_model = groq_cloud_tgmf.create_instance('llama3-70b-8192')\n",
    "# llama_8b_8192_generation_model = groq_cloud_tgmf.create_instance('llama3-8b-8192')\n",
    "\n",
    "# navi_gator_tgmf = TextGenerationModelFactory()\n",
    "# gpt_35_turbo_generation_model = navi_gator_tgmf.create_instance('gpt-3.5-turbo')\n",
    "# gpt_4_turbo_generation_model = navi_gator_tgmf.create_instance('gpt-4-turbo')\n",
    "# mixtral_87b_instruct_generation_model = navi_gator_tgmf.create_instance('mixtral-8x7b-instruct') \n",
    "\n",
    "tgmf = TextGenerationModelFactory()\n",
    "print(tgmf)\n",
    "llama_versatile_generation_model = tgmf.create_instance(model_name='llama-3.3-70b-versatile')\n",
    "llama_instant_generation_model = tgmf.create_instance('llama-3.1-8b-instant')\n",
    "llama_70b_8192_generation_model = tgmf.create_instance('llama3-70b-8192')\n",
    "llama_8b_8192_generation_model = tgmf.create_instance('llama3-8b-8192')\n",
    "\n",
    "gpt_35_turbo_generation_model = tgmf.create_instance('gpt-3.5-turbo')\n",
    "gpt_4_o_generation_model = tgmf.create_instance('gpt-4o')\n",
    "mixtral_87b_instruct_generation_model = tgmf.create_instance('mixtral-8x7b-instruct') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f78c6a",
   "metadata": {},
   "source": [
    "## LangChain Templates for Domain Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d32edc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_prediction_template = \"\"\"{prediction_properties}\n",
    "\n",
    "{prediction_requirements}\n",
    "\n",
    "{prediction_templates}\n",
    "\n",
    "{prediction_examples}\n",
    "\"\"\"\n",
    "\n",
    "full_prediction_prompt = PromptTemplate.from_template(full_prediction_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d420e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_properties_template = \"\"\"A prediction <p> = (<p_s>, <p_t>, <p_d>, <p_a>), where it consists of the following four properties:\n",
    "\n",
    "    1. <p_s>, any source entity in the {prediction_domain} domain.\n",
    "        - Can be a person (with a name) or a {prediction_domain} person such as a {prediction_domain} reporter, {prediction_domain} analyst, {prediction_domain} expert, {prediction_domain} top executive, {prediction_domain} senior level person, etc).\n",
    "        - Can only be an organization that is associated with the {prediction_domain} prediction.\n",
    "    2. <p_t>, any target entity in the {prediction_domain} domain.\n",
    "\t    - Can be a person (with a name) or a {prediction_domain} person such as a {prediction_domain} reporter, {prediction_domain} analyst, {prediction_domain} expert, {prediction_domain} top executive, {prediction_domain} senior level person, etc).\n",
    "        - Can only be an organization that is associated with the {prediction_domain} prediction.\n",
    "    3. <p_d>, date or time range when <p> is expected to come to fruition or when one should observe the <p>.\n",
    "        - Forecast can range from a second to anytime in the future.\n",
    "        - Answers the questions: \"How far to go out from today?\" or \"Where to stop?\".\n",
    "    4. <p_a>, {prediction_domain} prediction attribute.\n",
    "        - Characteristics of a domain-specific attributes such as various quantifiable metrics relevant to the {prediction_domain} domain.\n",
    "        - Some examples are {prediction_domain_attribute}.  \n",
    "\"\"\"\n",
    "prediction_properties_prompt = PromptTemplate.from_template(prediction_properties_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95548954",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_requirements_template = \"\"\"{prediction_domain} requirements to use for each prediction:\n",
    "\n",
    "    - Should be based on real-world {prediction_domain} data and not hallucinate.\n",
    "    - Only a simple sentence (prediction) (and NOT compounding using \"and\" or \"or\").\n",
    "    - Should diversify all four properties of the prediction (<p>) as in change and not use same for <p_s>, <p_t>, <p_d>, <p_a>.\n",
    "    - Should use synonyms to predict such as forecasts, speculates, foresee, envision, etc., and not use any of them more than ten times.\n",
    "    - The prediction should be unique and not repeated.\n",
    "    - Do not number the predictions.\n",
    "    - Do not say, \"As the {prediction_domain}, I will generate company-based {prediction_domain} predictions using the provided templates.\" or anything similar.\n",
    "    - Use the five different templates and examples provided.\n",
    "    - Change how the current date (<p_d>) written in the prediction with examples of (1) Wednesday, August 21, 2024; (2) Wed, August 21, 2024; (3) 08/21/2024; (4) 08/21/2024; (5) 21/08/2024; (6) 21 August 2024; (7) 2024/08/21; (8) 2024-08-21; (9) August 21, 2024; (10) Aug 21, 2024; (11) 21 August 2024, (12) 21 Aug 2024, Q3 of 2027, 2029 of Q3, etc (with removing day of week).\n",
    "    {domain_requirements}\n",
    "    - Do not say, \"Here are {predictions_N} unique {prediction_domain} predictions based on the provided templates and examples:\" in the prompt.\n",
    "    - Do not use any of the examples in the prompt.\n",
    "    - In front of every prodiction, put the template number in the format of \"T1:\", \"T2:\", etc. and do not number them like \"1.\", \"2.\", etc.\n",
    "    - Do not put template number on line by itself. Always pair with a prediction.\n",
    "    - Disregard brackets: \"<>\"\n",
    "    - Should never say \"Here are {predictions_N} unique {prediction_domain} predictions based on the provided templates and examples:\" or \"Note: I've made sure to follow the guidelines and templates provided, and generated unique predictions that meet the requirements.\"\n",
    "    - Do not use person name of entity name more than once as in don't use name Joe as both the <p_s> and <p_t>, unless like Mr. Sach and Goldman Sach or Mr. Sam Walton and Sam's Club, etc.\n",
    "    - The source entity (<p_s>) is rarely the same as the target entity (<p_t>) and if same, the <p_s> is making a prediction on itself in the <p_t>.\n",
    "    - Should variate the slope of rise/increase/as much as, fall/decrease/as little as, change, stay stable, high/low chance/probability/degree of, etc.\n",
    "    - Should variate the prediction verbs such as will, would, be going to, should, etc.\n",
    "\"\"\"\n",
    "prediction_requirements_prompt = PromptTemplate.from_template(prediction_requirements_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07129ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_templates_template = \"\"\"Here are some {prediction_domain} templates:\n",
    "\n",
    "- {prediction_domain} template 1: <p_s> forecasts that the <p_a> at <p_t> will likely decrease in <p_d>.\n",
    "\n",
    "\"\"\"\n",
    "prediction_templates_prompt = PromptTemplate.from_template(prediction_templates_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e559ea64",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_examples_template = \"\"\"Here are some examples of {prediction_domain} predictions:\n",
    "\n",
    "{domain_examples}\n",
    "\n",
    "With the above, generate a unique set of {predictions_N} predictions. Think from the perspective of an {prediction_domain} analyst, expert, top executive, or senior level person.\"\"\"\n",
    "prediction_examples_prompt = PromptTemplate.from_template(prediction_examples_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad59deb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/78/9z0b45fx1xqbwxh8vk97lcfh0000gn/T/ipykernel_93812/3566428802.py:8: LangChainDeprecationWarning: This class is deprecated. Please see the docstring below or at the link for a replacement option: https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.pipeline.PipelinePromptTemplate.html\n",
      "  pipeline_prompt = PipelinePromptTemplate(\n"
     ]
    }
   ],
   "source": [
    "prediction_input_prompts = [\n",
    "    (\"prediction_properties\", prediction_properties_prompt),\n",
    "    (\"prediction_requirements\", prediction_requirements_prompt),\n",
    "    (\"prediction_templates\", prediction_templates_prompt),\n",
    "    (\"prediction_examples\", prediction_examples_prompt),\n",
    "]\n",
    "\n",
    "pipeline_prompt = PipelinePromptTemplate(\n",
    "    final_prompt=full_prediction_prompt, pipeline_prompts=prediction_input_prompts\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099b1ddb",
   "metadata": {},
   "source": [
    "## Generate Domain Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be6a146d",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_N = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd133022",
   "metadata": {},
   "source": [
    "### Generate Financial Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e2ddf01",
   "metadata": {},
   "outputs": [],
   "source": [
    "financial_attributes = \"\"\"stock price, net profit, revenue, operating cash flow, research and development expenses, operating income, gross profit.\"\"\"\n",
    "financial_requirements = \"\"\"- Should be based on real-world financial earnings reports.\n",
    "    - Suppose the time when $p$ was made is during any earning season.\n",
    "    - Include stocks from all sectors such as consumer staples, energy, finance, health care, industrials, materials, media, real estate, retail, technology, utilities, defense, etc.\n",
    "    - Include the US Dollar sign ($) before or USD after the amount of the financial attribute.\"\"\"\n",
    "\n",
    "financial_examples = \"\"\"\n",
    "    1. Detravious, an investor forecasts that the stock price at Apple will likely decrease in 2025 Q1.\n",
    "    2. Ava Lee predicts that the operating cash flow at ExxonMobil should decrease in 03/21/2025 to 08/21/2025.\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8d47f9ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A prediction <p> = (<p_s>, <p_t>, <p_d>, <p_a>), where it consists of the following four properties:\n",
      "\n",
      "    1. <p_s>, any source entity in the financial domain.\n",
      "        - Can be a person (with a name) or a financial person such as a financial reporter, financial analyst, financial expert, financial top executive, financial senior level person, etc).\n",
      "        - Can only be an organization that is associated with the financial prediction.\n",
      "    2. <p_t>, any target entity in the financial domain.\n",
      "\t    - Can be a person (with a name) or a financial person such as a financial reporter, financial analyst, financial expert, financial top executive, financial senior level person, etc).\n",
      "        - Can only be an organization that is associated with the financial prediction.\n",
      "    3. <p_d>, date or time range when <p> is expected to come to fruition or when one should observe the <p>.\n",
      "        - Forecast can range from a second to anytime in the future.\n",
      "        - Answers the questions: \"How far to go out from today?\" or \"Where to stop?\".\n",
      "    4. <p_a>, financial prediction attribute.\n",
      "        - Characteristics of a domain-specific attributes such as various quantifiable metrics relevant to the financial domain.\n",
      "        - Some examples are stock price, net profit, revenue, operating cash flow, research and development expenses, operating income, gross profit..  \n",
      "\n",
      "\n",
      "financial requirements to use for each prediction:\n",
      "\n",
      "    - Should be based on real-world financial data and not hallucinate.\n",
      "    - Only a simple sentence (prediction) (and NOT compounding using \"and\" or \"or\").\n",
      "    - Should diversify all four properties of the prediction (<p>) as in change and not use same for <p_s>, <p_t>, <p_d>, <p_a>.\n",
      "    - Should use synonyms to predict such as forecasts, speculates, foresee, envision, etc., and not use any of them more than ten times.\n",
      "    - The prediction should be unique and not repeated.\n",
      "    - Do not number the predictions.\n",
      "    - Do not say, \"As the financial, I will generate company-based financial predictions using the provided templates.\" or anything similar.\n",
      "    - Use the five different templates and examples provided.\n",
      "    - Change how the current date (<p_d>) written in the prediction with examples of (1) Wednesday, August 21, 2024; (2) Wed, August 21, 2024; (3) 08/21/2024; (4) 08/21/2024; (5) 21/08/2024; (6) 21 August 2024; (7) 2024/08/21; (8) 2024-08-21; (9) August 21, 2024; (10) Aug 21, 2024; (11) 21 August 2024, (12) 21 Aug 2024, Q3 of 2027, 2029 of Q3, etc (with removing day of week).\n",
      "    - Should be based on real-world financial earnings reports.\n",
      "    - Suppose the time when $p$ was made is during any earning season.\n",
      "    - Include stocks from all sectors such as consumer staples, energy, finance, health care, industrials, materials, media, real estate, retail, technology, utilities, defense, etc.\n",
      "    - Include the US Dollar sign ($) before or USD after the amount of the financial attribute.\n",
      "    - Do not say, \"Here are 2 unique financial predictions based on the provided templates and examples:\" in the prompt.\n",
      "    - Do not use any of the examples in the prompt.\n",
      "    - In front of every prodiction, put the template number in the format of \"T1:\", \"T2:\", etc. and do not number them like \"1.\", \"2.\", etc.\n",
      "    - Do not put template number on line by itself. Always pair with a prediction.\n",
      "    - Disregard brackets: \"<>\"\n",
      "    - Should never say \"Here are 2 unique financial predictions based on the provided templates and examples:\" or \"Note: I've made sure to follow the guidelines and templates provided, and generated unique predictions that meet the requirements.\"\n",
      "    - Do not use person name of entity name more than once as in don't use name Joe as both the <p_s> and <p_t>, unless like Mr. Sach and Goldman Sach or Mr. Sam Walton and Sam's Club, etc.\n",
      "    - The source entity (<p_s>) is rarely the same as the target entity (<p_t>) and if same, the <p_s> is making a prediction on itself in the <p_t>.\n",
      "    - Should variate the slope of rise/increase/as much as, fall/decrease/as little as, change, stay stable, high/low chance/probability/degree of, etc.\n",
      "    - Should variate the prediction verbs such as will, would, be going to, should, etc.\n",
      "\n",
      "\n",
      "Here are some financial templates:\n",
      "\n",
      "- financial template 1: <p_s> forecasts that the <p_a> at <p_t> will likely decrease in <p_d>.\n",
      "\n",
      "\n",
      "\n",
      "Here are some examples of financial predictions:\n",
      "\n",
      "\n",
      "    1. Detravious, an investor forecasts that the stock price at Apple will likely decrease in 2025 Q1.\n",
      "    2. Ava Lee predicts that the operating cash flow at ExxonMobil should decrease in 03/21/2025 to 08/21/2025.\n",
      " \n",
      "\n",
      "With the above, generate a unique set of 2 predictions. Think from the perspective of an financial analyst, expert, top executive, or senior level person.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "financial_input_dict = {\n",
    "    \"prediction_domain\": \"financial\",\n",
    "    \"prediction_domain_attribute\": financial_attributes,\n",
    "    \"domain_requirements\": financial_requirements,\n",
    "    \"domain_examples\": financial_examples,\n",
    "    \"predictions_N\": predictions_N\n",
    "}\n",
    "financial_prompt_output = pipeline_prompt.format(**financial_input_dict)\n",
    "print(financial_prompt_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d02699a",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_batches = 1\n",
    "# text_generation_models = [llama_instant_generation_model]\n",
    "# text_generation_models = [llama_versatile_generation_model]\n",
    "text_generation_models = [llama_versatile_generation_model, llama_instant_generation_model, llama_70b_8192_generation_model, \n",
    "                          llama_8b_8192_generation_model, gpt_35_turbo_generation_model, gpt_4gpt_4_o_generation_model_turbo_generation_model, \n",
    "                          mixtral_87b_instruct_generation_model]\n",
    "\n",
    "# text_generation_models = [llama_versatile_generation_model, llama_instant_generation_model, llama_70b_8192_generation_model, \n",
    "#                           llama_8b_8192_generation_model, gpt_35_turbo_generation_model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1546368e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finance --- <text_generation_models.LlamaVersatileTextGenerationModel object at 0x35122c0b0>\n",
      "finance --- <text_generation_models.LlamaInstantTextGenerationModel object at 0x3512a9040>\n",
      "finance --- <text_generation_models.Llama70B8192TextGenerationModel object at 0x350fcdc10>\n",
      "finance --- <text_generation_models.Llama8B8192TextGenerationModel object at 0x3512ab4d0>\n",
      "finance --- <text_generation_models.Gpt35TurboTextGenerationModel object at 0x3512cc770>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:03<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finance --- <text_generation_models.Gpt4TurboTextGenerationModel object at 0x3512ce2a0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'error': {'message': \"{'error': '/chat/completions: Invalid model name passed in model=gpt-4-turbo. Call `/v1/models` to view available models for your key.'}\", 'type': 'None', 'param': 'None', 'code': '400'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mBadRequestError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      2\u001b[39m prediction_prompt_outputs = {\n\u001b[32m      3\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mfinance\u001b[39m\u001b[33m\"\u001b[39m: financial_prompt_output,\n\u001b[32m      4\u001b[39m }\n\u001b[32m      5\u001b[39m prediction_label = \u001b[32m1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m batched_predictions_df = \u001b[43mtgmf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbatch_generate_predictions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mN_batches\u001b[49m\u001b[43m=\u001b[49m\u001b[43mN_batches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m                                \u001b[49m\u001b[43mtext_generation_models\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext_generation_models\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m                                \u001b[49m\u001b[43mdomains\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprediction_domains\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m                                \u001b[49m\u001b[43mprompt_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprediction_prompt_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m                                \u001b[49m\u001b[43msentence_label\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprediction_label\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Development/research_labs/uf_ds/predictions/prediction_correctness/../text_generation_models.py:194\u001b[39m, in \u001b[36mTextGenerationModelFactory.batch_generate_predictions\u001b[39m\u001b[34m(self, N_batches, text_generation_models, domains, prompt_outputs, sentence_label)\u001b[39m\n\u001b[32m    191\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdomain\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m --- \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtext_generation_model\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    193\u001b[39m prompt_output = prompt_outputs[domain]\n\u001b[32m--> \u001b[39m\u001b[32m194\u001b[39m model_df = \u001b[43mtext_generation_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_predictions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m=\u001b[49m\u001b[43msentence_label\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdomain\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdomain\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    195\u001b[39m model_df[\u001b[33m\"\u001b[39m\u001b[33mBatch Index\u001b[39m\u001b[33m\"\u001b[39m] = batch_idx\n\u001b[32m    197\u001b[39m batch_dfs.append(model_df)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Development/research_labs/uf_ds/predictions/prediction_correctness/../text_generation_models.py:160\u001b[39m, in \u001b[36mTextGenerationModelFactory.generate_predictions\u001b[39m\u001b[34m(self, prompt_template, label, domain)\u001b[39m\n\u001b[32m    138\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Generate a completion response and return as a DataFrame.\u001b[39;00m\n\u001b[32m    139\u001b[39m \n\u001b[32m    140\u001b[39m \u001b[33;03mParameters:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    157\u001b[39m \u001b[33;03m    The generated completion response formatted as a DataFrame.\u001b[39;00m\n\u001b[32m    158\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    159\u001b[39m \u001b[38;5;66;03m# Generate the raw prediction text\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m160\u001b[39m raw_text = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mchat_completion\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43muser\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_template\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    162\u001b[39m \u001b[38;5;66;03m# Parse the raw text into structured data (assuming a consistent format)\u001b[39;00m\n\u001b[32m    163\u001b[39m predictions = []\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Development/research_labs/uf_ds/predictions/prediction_correctness/../text_generation_models.py:129\u001b[39m, in \u001b[36mTextGenerationModelFactory.chat_completion\u001b[39m\u001b[34m(self, messages)\u001b[39m\n\u001b[32m    106\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mchat_completion\u001b[39m(\u001b[38;5;28mself\u001b[39m, messages: List[Dict]) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m    107\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Generate a chat completion response.\u001b[39;00m\n\u001b[32m    108\u001b[39m \u001b[33;03m    \u001b[39;00m\n\u001b[32m    109\u001b[39m \u001b[33;03m    Parameters:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    126\u001b[39m \u001b[33;03m        The generated chat completion response.\u001b[39;00m\n\u001b[32m    127\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m129\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompletions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    130\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    131\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    132\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    133\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    134\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    135\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response.choices[\u001b[32m0\u001b[39m].message.content\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/predictions/lib/python3.12/site-packages/openai/_utils/_utils.py:279\u001b[39m, in \u001b[36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    277\u001b[39m             msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[32m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    278\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m279\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/predictions/lib/python3.12/site-packages/openai/resources/chat/completions.py:850\u001b[39m, in \u001b[36mCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    808\u001b[39m \u001b[38;5;129m@required_args\u001b[39m([\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m], [\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m    809\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m    810\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    847\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = NOT_GIVEN,\n\u001b[32m    848\u001b[39m ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[32m    849\u001b[39m     validate_response_format(response_format)\n\u001b[32m--> \u001b[39m\u001b[32m850\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    851\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/chat/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    852\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    853\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    854\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    855\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    856\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43maudio\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    857\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrequency_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    858\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunction_call\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    859\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunctions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    860\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_bias\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    861\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    862\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_completion_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    863\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    864\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    865\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodalities\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    866\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    867\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    868\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprediction\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    869\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpresence_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    870\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_effort\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    871\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    872\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    873\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    880\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    881\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    882\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    883\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    884\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    885\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    886\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    887\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    888\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    889\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    890\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    891\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    892\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    893\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/predictions/lib/python3.12/site-packages/openai/_base_client.py:1283\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1269\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1270\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1271\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1278\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1279\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1280\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1281\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1282\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1283\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/predictions/lib/python3.12/site-packages/openai/_base_client.py:960\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[39m\n\u001b[32m    957\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    958\u001b[39m     retries_taken = \u001b[32m0\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m960\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    961\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    962\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    963\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    964\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    965\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    966\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/predictions/lib/python3.12/site-packages/openai/_base_client.py:1064\u001b[39m, in \u001b[36mSyncAPIClient._request\u001b[39m\u001b[34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[39m\n\u001b[32m   1061\u001b[39m         err.response.read()\n\u001b[32m   1063\u001b[39m     log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1064\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1066\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._process_response(\n\u001b[32m   1067\u001b[39m     cast_to=cast_to,\n\u001b[32m   1068\u001b[39m     options=options,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1072\u001b[39m     retries_taken=retries_taken,\n\u001b[32m   1073\u001b[39m )\n",
      "\u001b[31mBadRequestError\u001b[39m: Error code: 400 - {'error': {'message': \"{'error': '/chat/completions: Invalid model name passed in model=gpt-4-turbo. Call `/v1/models` to view available models for your key.'}\", 'type': 'None', 'param': 'None', 'code': '400'}}"
     ]
    }
   ],
   "source": [
    "prediction_domains = [\"finance\"]\n",
    "prediction_prompt_outputs = {\n",
    "    \"finance\": financial_prompt_output,\n",
    "}\n",
    "prediction_label = 1\n",
    "\n",
    "batched_predictions_df = tgmf.batch_generate_predictions(N_batches=N_batches, \n",
    "                                text_generation_models=text_generation_models, \n",
    "                                domains=prediction_domains,\n",
    "                                prompt_outputs=prediction_prompt_outputs,\n",
    "                                sentence_label=prediction_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14728864",
   "metadata": {},
   "outputs": [],
   "source": [
    "batched_predictions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5834805",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('max_colwidth', 800)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "batched_predictions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cde3a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df = DataProcessing.concat_dfs(batched_predictions_df)\n",
    "predictions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40358be",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_directory = '../data/prediction_logs/'\n",
    "file_name = 'test.log'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec896b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = LogData(log_directory, file_name)\n",
    "logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a857cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_output_path = os.path.join(log_directory, 'from_dataframe.csv')\n",
    "logger.dataframe_to_csv(predictions_df, csv_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260bfc71",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_input_path = os.path.join(log_directory, 'from_dataframe.csv')\n",
    "file_name = 'from_csv.log'\n",
    "logger.csv_to_log(csv_input_path, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d124d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_output_from_log_path = os.path.join(log_directory, 'from_log.csv')\n",
    "ignore_patterns = ['INFO', 'DEBUG', 'ERROR', 'WARNING', 'CSV Row: '] # Example patterns to ignore\n",
    "\n",
    "logger.log_to_csv(file_name, csv_output_from_log_path, ignore_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20463eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_input_from_log_path = os.path.join(log_directory, 'from_dataframe.csv')\n",
    "df_from_log_csv = logger.csv_to_dataframe(csv_input_from_log_path)\n",
    "\n",
    "if df_from_log_csv is not None:\n",
    "    print(\"\\nDataFrame created from log CSV:\")\n",
    "    print(df_from_log_csv)\n",
    "else:\n",
    "    print(\"\\nFailed to create DataFrame from log CSV. Check the log file for errors.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be029e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_from_log_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cad7888",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "predictions",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
